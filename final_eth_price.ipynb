{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2'\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('Copper_forecasting_data.csv')\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = data.drop([\"y\"], axis = 1)\n",
    "X = sc.fit_transform(X)\n",
    "y = data[\"y\"] / 1000\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "batch_size = 30\n",
    "\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:] \n",
    "\n",
    "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.float32)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        X_, y_ = self.X[index], self.y[index]\n",
    "        return X_, y_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_train.to(device), y_train.to(device)), batch_size = batch_size, shuffle=False, drop_last = False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_test.to(device), y_test.to(device)), batch_size = batch_size, shuffle=False, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.Reorg import *\n",
    "from module.Weight_tune import *\n",
    "\n",
    "# weight tune module\n",
    "hidden_dim = 50\n",
    "model = TwoLayerNet(18, hidden_dim, 1)\n",
    "\n",
    "config_w_tune = {\n",
    "    \"epochs\": 100,\n",
    "    \"epsilon\": 10, # 13 for getting acceptable wt\n",
    "    \"lr_lowerbound\": 1e-6,\n",
    "    \"optimizer\": optim.Adam(model.parameters(), lr = 0.01),\n",
    "    \"criterion\": torch.nn.MSELoss(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module_weight_EU_LG_UA\n",
    "- To acheive a acceptable SLFN\n",
    "- store in acceptable/wt.pth\n",
    "- Already acheived, skip this process, jump to reorganise module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Acceptable: iteratively random initialisation of weight tuning module\n",
    "while True:\n",
    "    acceptable, model, train_loss, test_loss = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_w_tune)\n",
    "    if acceptable == True:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Acceptable\n",
    "acceptable, model, train_loss, test_loss = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_w_tune)\n",
    "eps, pred = eps_for_each(train_loader, model)\n",
    "acceptable, max(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for weight tuning module: 767.6356582641602\n"
     ]
    }
   ],
   "source": [
    "# test loss\n",
    "loss_test = 0\n",
    "criterion = nn.MSELoss()\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = model(X)\n",
    "    loss_test_ = criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "loss_test /= len(test_loader)\n",
    "print(f\"Test loss for weight tuning module: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable SLFN exist in 'acceptable/wt.pth'.\n"
     ]
    }
   ],
   "source": [
    "# check if acceptable SLFN exist\n",
    "import os\n",
    "acceptable_wt_path = 'acceptable/wt.pth'\n",
    "if os.path.exists(acceptable_wt_path):\n",
    "    print(f\"Acceptable SLFN exist in '{acceptable_wt_path}'.\")\n",
    "    model = torch.load(acceptable_wt_path)\n",
    "else:\n",
    "    print(f\"Acceptable SLFN not exist in '{acceptable_wt_path}'.\")\n",
    "    model = None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module reorganising_EU_LG_UA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regularise with Pytorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md)\n",
    "- L7 p 63\n",
    "- Classmate p 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([25.3450], grad_fn=<UnbindBackward0>),\n",
       " TwoLayerNet(\n",
       "   (layer_1): Linear(in_features=9, out_features=164, bias=True)\n",
       "   (layer_out): Linear(in_features=164, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_model = torch.load(\"acceptable/Cram.pth\")\n",
    "eps, y_pred = eps_for_each(train_loader, previous_model)\n",
    "max(eps), previous_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.Reorg import *\n",
    "# reorganising module\n",
    "\"\"\"\n",
    "# these config need to consist to config in weight tune module:\n",
    "#   1. hidden_dim\n",
    "#   2. epochs\n",
    "#   3. eps\n",
    "#   4. criterion\n",
    "#   5. lr_bounds\n",
    "# Note: hidden dim should check the previous model \n",
    "\"\"\"\n",
    "config_reorg  = {\n",
    "    \"hidden_dim\": 164,\n",
    "    \"epochs\": 100,\n",
    "    \"criterion\": nn.MSELoss(),     # loss function\n",
    "    \"lr_reg\": 0.1,                # learning rate for regularisation\n",
    "    \"lr_w_tune\": 0.1,             # learning rate for weight tuning\n",
    "    \"lr_bound_reg\": 1e-6,          # lower bound of learning rate regularisation\n",
    "    \"lr_bound_w_tune\": 1e-6,       # lower bound of learning rate weight tuning\n",
    "    \"eps_reg\": max(eps),               # if regular eps < eps_reg: accept the model\n",
    "    \"eps_w_tune\": max(eps),            # if weight tune eps < eps_w_tune: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable SLFN exist in 'acceptable/Cram.pth'.\n",
      "=================== reorganising ===================\n",
      "[ 0.00%] ------------> Checking nodes...\n",
      "TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=9, out_features=164, bias=True)\n",
      "  (layer_out): Linear(in_features=164, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "    --> Start regularising_EU_LG_UA\n",
      "| 0.00%| Epoch 0\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eps_for_each' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Regular module\u001b[39;00m\n\u001b[0;32m      2\u001b[0m reorg \u001b[38;5;241m=\u001b[39m reorganising(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCram\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loader, test_loader, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_reorg)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mreorg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreorganising\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\NCCU new learning algorithm\\Copper\\module\\Reorg.py:346\u001b[0m, in \u001b[0;36mreorganising.reorganising\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    --> Start regularising_EU_LG_UA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    344\u001b[0m check_n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    345\u001b[0m acceptable, train_loss_list, test_loss_list \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregularising_EU_LG_UA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_temp/Reorg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# A new model trimmed from regualr module model\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Use the trim model in weigt tuning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\NCCU new learning algorithm\\Copper\\module\\Reorg.py:231\u001b[0m, in \u001b[0;36mreorganising.regularising_EU_LG_UA\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m     test_loss_list\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# stopping criteria 1\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m eps, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43meps_for_each\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_reg:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax eps sqaure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(eps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eps_for_each' is not defined"
     ]
    }
   ],
   "source": [
    "# Regular module\n",
    "reorg = reorganising(\"Cram\", train_loader, test_loader, **config_reorg)\n",
    "reorg.reorganising()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4264470848.0\n"
     ]
    }
   ],
   "source": [
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = reorg.model(X)\n",
    "    loss_test_ = reorg.criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ========= How to use weight tune, regular, and trim =========\\n# Weight tune ----------------------------------\\nmodel = TwoLayerNet(9, config[\"hidden_dim\"], 1)\\noptimizer = optim.SGD(model.parameters(), lr=config[\"lr_w_tune\"])\\nreorg.module_weight_EU_LG_UA(model, optimizer)\\n# Regular --------------------------------------\\nacceptable, model, train_loss, test_loss = reorg.regularising_EU_LG_UA()\\n# Trim model --------------------------------------\\nmodel_ = reorg.trim_model_nodes(2)\\nprint(reorg.model, model_)\\n# Reorg ----------------------------------------------\\nreorg.reorganising()\\n# final model ------------------------------------------\\nreorg.model\\n# test final model -------------------------------------\\nloss_test = 0\\nfor _, (X, y) in enumerate(test_loader):\\n    pred_test = reorg.model(X)\\n    loss_test_ = reorg.criterion(pred_test, y)\\n    loss_test += loss_test_.item()\\nprint(loss_test)\\n# load reorganise trained model----------------------------------\\nmodel_reorg = torch.load(\"final_model/Reorg\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ========= How to use weight tune, regular, and trim =========\n",
    "# Weight tune ----------------------------------\n",
    "model = TwoLayerNet(9, config[\"hidden_dim\"], 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=config[\"lr_w_tune\"])\n",
    "reorg.module_weight_EU_LG_UA(model, optimizer)\n",
    "# Regular --------------------------------------\n",
    "acceptable, model, train_loss, test_loss = reorg.regularising_EU_LG_UA()\n",
    "# Trim model --------------------------------------\n",
    "model_ = reorg.trim_model_nodes(2)\n",
    "print(reorg.model, model_)\n",
    "# Reorg ----------------------------------------------\n",
    "reorg.reorganising()\n",
    "# final model ------------------------------------------\n",
    "reorg.model\n",
    "# test final model -------------------------------------\n",
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = reorg.model(X)\n",
    "    loss_test_ = reorg.criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "print(loss_test)\n",
    "# load reorganise trained model----------------------------------\n",
    "model_reorg = torch.load(\"final_model/Reorg\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (layer_1): Linear(in_features=9, out_features=28, bias=True)\n",
       "  (layer_out): Linear(in_features=28, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reorg = torch.load(\"acceptable/Reorg.pth\")\n",
    "model_reorg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module_ReLU_RI_SO_RE_MU: Cramming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: L9, p5 | L8, p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cramming(nn.Module):\n",
    "    def __init__(self, train_loader, X_train, y_train, eps_bound, s):\n",
    "        \n",
    "        \"\"\"\n",
    "        model: \n",
    "        train_loader\n",
    "        X_train:\n",
    "        y_train:\n",
    "        eps_bound: maximum epsilon accepted for EACH sample\n",
    "                   as epsilon in weight tuning module and eps_reg, eps_w_tune in module above\n",
    "        s: float (tiny) that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        super(cramming, self).__init__()\n",
    "        if os.path.exists(\"unacceptable/wt.pth\"):\n",
    "            self.model = torch.load(\"unacceptable/wt.pth\")\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(f\"The unacceptable model does not exist.\")\n",
    "        self.input_dim = self.model.layer_1.weight.data.shape[1]\n",
    "        self.hidden_dim = self.model.layer_1.weight.data.shape[0]\n",
    "        self.train_loader = train_loader\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.eps_bound = eps_bound\n",
    "        (\n",
    "        self.eps,\n",
    "        self.y_pred\n",
    "        ) = eps_for_each(train_loader, self.model)\n",
    "        self.s = s\n",
    "        self.ks = torch.where(self.eps > self.eps_bound)[0]\n",
    "    \n",
    "\n",
    "    def cram(self):\n",
    "        for i, k in enumerate(self.ks):\n",
    "            print(f\"cramming sample {k} |{i/len(self.ks)*100:.2f}% total of {len(self.ks)}\")\n",
    "            r = self.cram_find_r(k)\n",
    "            self.cram_add_node(r, k)\n",
    "        torch.save(self.model, \"acceptable/Cram.pth\")\n",
    "\n",
    "    def cram_find_r(self, k): \n",
    "        # L9, isolation R2: p39, carm: p.54, for multiple case: p.60?\n",
    "        \"\"\"\n",
    "        k: k sample to cram (unaccepted sample with too large epsilon)\n",
    "        ==========\n",
    "        outputs\n",
    "        r: vector that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        print(\"find r: r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\", end = \"\\r\")\n",
    "\n",
    "        Xc_Xk = []\n",
    "\n",
    "        k = 1\n",
    "        X_no_k = torch.cat([X_train[:k], X_train[k+1:]], dim = 0)\n",
    "        if torch.any(torch.all(X_no_k == X_train[k], dim=1)):\n",
    "            print(\"If X_train[k] in X_train: yes. check again\")\n",
    "        \n",
    "        n = 0\n",
    "        while True:\n",
    "            n+=1\n",
    "            print(f\"try vector {n}\", end=\"\\r\")\n",
    "\n",
    "            r = torch.rand(self.input_dim)            \n",
    "            dots = ((X_no_k - X_train[k]) @ r.T) \n",
    "            print(max((self.s + dots) * (self.s - dots)))\n",
    "            \n",
    "            if (torch.sum(dots == 0) == 0) and (max((self.s + dots) * (self.s - dots)) < 0):\n",
    "                return r\n",
    "            \n",
    "    def cram_add_node(self, r, k):\n",
    "        \"\"\"\n",
    "        k: k sample to cram (unaccepted sample with too large epsilon)\n",
    "        r: vector that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        new_model = TwoLayerNet(9, self.hidden_dim+3, 1)\n",
    "\n",
    "        param = self.model.state_dict()\n",
    "        for name in model.state_dict():\n",
    "            if name == 'layer_1.weight':\n",
    "                # First node\n",
    "                new_w = torch.cat([param[name], r.reshape(1, -1)], dim = 0)\n",
    "                # Second node\n",
    "                new_w = torch.cat([new_w, r.reshape(1, -1)], dim = 0)\n",
    "                # Third node\n",
    "                new_w = torch.cat([new_w, r.reshape(1, -1)], dim = 0)\n",
    "\n",
    "                new_model.layer_1.weight.data = new_w\n",
    "                    \n",
    "            if name == 'layer_1.bias':\n",
    "                # First node\n",
    "                node_add = self.s - torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([param[name], node_add.reshape(1)], dim = 0)\n",
    "                # Second node\n",
    "                node_add = (-1) * torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([new_b, node_add.reshape(1)], dim = 0)\n",
    "                # Third node\n",
    "                node_add = (-1)*self.s - torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([new_b, node_add.reshape(1)], dim = 0)\n",
    "\n",
    "                new_model.layer_1.bias.data = new_b\n",
    "\n",
    "            if name == 'layer_out.weight':\n",
    "                \"\"\"\n",
    "                # the base of Xk = (yk - prediction yk)/s\n",
    "                aik = nn.ReLU()(model.layer_1.weight.data @ X_train[k].T)\n",
    "                out_weight = model.layer_out.weight.data.reshape(-1)\n",
    "                out_bias = model.layer_out.bias.data.reshape(-1)\n",
    "                base = (y_train[k] - out_bias - torch.dot(out_weight, aik))/s\n",
    "                \"\"\"\n",
    "                base = (self.y_train[k] - self.model(self.X_train[k]))/self.s\n",
    "                # First node\n",
    "                new_w_o = torch.cat([param[name], base.reshape(1, 1)], dim = 1)\n",
    "                # Second node\n",
    "                new_w_o = torch.cat([new_w_o, ((-2)*base).reshape(1, 1)], dim = 1)\n",
    "                # Third node\n",
    "                new_w_o = torch.cat([new_w_o, base.reshape(1, 1)], dim = 1)\n",
    "\n",
    "                new_model.layer_out.weight.data = new_w_o\n",
    "\n",
    "        self.model = new_model\n",
    "        self.hidden_dim +=3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, vs cram.model\n",
    "model = torch.load(\"unacceptable/wt.pth\")\n",
    "config_cramming = {\n",
    "    \"eps_bound\": np.percentile(eps.detach().numpy(), 90), \n",
    "    \"s\": 0.1,\n",
    "}\n",
    "cram = cramming(train_loader, X_train, y_train, **config_cramming)\n",
    "cram.cram()\n",
    "cram.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0107],\n",
       "        [0.1477],\n",
       "        [0.0811],\n",
       "        [0.1293],\n",
       "        [0.1260],\n",
       "        [0.3069]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps, y_pred = eps_for_each(train_loader, model)\n",
    "index = torch.where(eps > .01)[0]\n",
    "eps, y_pred = eps_for_each(train_loader, cram.model)\n",
    "eps[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743.7243156433105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = cram.model(X)\n",
    "    loss_test_ = nn.MSELoss()(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "loss_test /= len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0073,  0.0435,  0.0546, -0.2061, -0.1659,  0.1690, -0.1748,  0.1323,\n",
       "          -0.1250, -0.1714,  0.0733, -0.0514,  0.0334,  0.1305, -0.0112,  0.1752,\n",
       "          -0.0449,  0.0449, -0.0729, -0.1455]]),\n",
       " tensor([[ 7.2531e-03,  4.3481e-02,  5.4579e-02, -2.0614e-01, -1.6595e-01,\n",
       "           1.6896e-01, -1.7480e-01,  1.3230e-01, -1.2500e-01, -1.7137e-01,\n",
       "           7.3287e-02, -5.1439e-02,  3.3421e-02,  1.3048e-01, -1.1244e-02,\n",
       "           1.7522e-01, -4.4895e-02,  4.4885e-02, -7.2886e-02, -1.4554e-01,\n",
       "           2.1068e+04, -4.2136e+04,  2.1068e+04,  1.0528e+05, -2.1057e+05,\n",
       "           1.0528e+05,  1.9568e+04, -3.9137e+04,  1.9568e+04, -1.7484e+04,\n",
       "           3.4968e+04, -1.7484e+04, -4.7019e+04,  9.4038e+04, -4.7019e+04,\n",
       "           1.1945e+04, -2.3890e+04,  1.1945e+04]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights\n",
    "model.layer_1.weight.data, new_model.layer_1.weight.data.shape\n",
    "model.layer_1.bias.data, new_model.layer_1.bias.data\n",
    "model.layer_out.weight.data, new_model.layer_out.weight.data\n",
    "# check grad\n",
    "# model.layer_1.weight.requires_grad, new_model.layer_1.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 76.33348382392433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.33348382392433"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model, iterator, criterion):\n",
    "\n",
    "    model.eval()     # Enter Evaluation Mode\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (ii, targets) in enumerate(iterator):\n",
    "\n",
    "            # move to GPU if necessary\n",
    "            # ii, targets = ii.to(device), targets.to(device)\n",
    "            \n",
    "            # generate prediction\n",
    "            preds = model(ii)\n",
    "            preds = preds.view(-1)\n",
    "            \n",
    "            # convert target tensor to long\n",
    "            targets = targets.long()\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            # record training losses\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # print completed result\n",
    "    print('test_loss: %s' % (test_loss))\n",
    "    return test_loss\n",
    "criterion = torch.nn.MSELoss()\n",
    "test(model = model, iterator = train_loader, criterion = criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(50,10,1)\n",
    "hidden_dim = 10\n",
    "epochs_reg = 50\n",
    "epochs_weight = 50\n",
    "iterator = train_loader \n",
    "optimizer_reg = optim.Adam(model.parameters(), lr = 0.005)\n",
    "criterion_reg = torch.nn.MSELoss()\n",
    "optimizer_weight = optim.Adam(model.parameters(), lr = 0.005)\n",
    "criterion_weight = torch.nn.MSELoss()\n",
    "lr_reg = 0.01\n",
    "lr_weight = 0.005\n",
    "epsilon_reg = 0.1\n",
    "epsilon_lr = 0.001\n",
    "epsilon_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_EU_LG===== Epoch 0 =====\n",
      "train_loss: 13.598101888317615\n",
      "module_EU_LG===== Epoch 1 =====\n",
      "train_loss: 0.22697472048457712\n",
      "module_EU_LG===== Epoch 2 =====\n",
      "train_loss: 0.10631587615353055\n",
      "module_EU_LG===== Epoch 3 =====\n",
      "train_loss: 0.06641909293830395\n",
      "module_EU_LG===== Epoch 4 =====\n",
      "train_loss: 0.04847166124091018\n",
      "module_EU_LG===== Epoch 5 =====\n",
      "train_loss: 0.039028783518006094\n",
      "module_EU_LG===== Epoch 6 =====\n",
      "train_loss: 0.0358568191586528\n",
      "module_EU_LG===== Epoch 7 =====\n",
      "train_loss: 0.03274400309601333\n",
      "module_EU_LG===== Epoch 8 =====\n",
      "train_loss: 0.030239086743677035\n",
      "module_EU_LG===== Epoch 9 =====\n",
      "train_loss: 0.02912202855804935\n",
      "module_EU_LG===== Epoch 10 =====\n",
      "train_loss: 0.02663213685445953\n",
      "module_EU_LG===== Epoch 11 =====\n",
      "train_loss: 0.028520387582830153\n",
      "module_EU_LG===== Epoch 12 =====\n",
      "train_loss: 0.026817812817171216\n",
      "module_EU_LG===== Epoch 13 =====\n",
      "train_loss: 0.027025780378608033\n",
      "module_EU_LG===== Epoch 14 =====\n",
      "train_loss: 0.02852823601278942\n",
      "module_EU_LG===== Epoch 15 =====\n",
      "train_loss: 0.02591095757088624\n",
      "module_EU_LG===== Epoch 16 =====\n",
      "train_loss: 0.02479221305839019\n",
      "module_EU_LG===== Epoch 17 =====\n",
      "train_loss: 0.026358386319770943\n",
      "module_EU_LG===== Epoch 18 =====\n",
      "train_loss: 0.027986457032966428\n",
      "module_EU_LG===== Epoch 19 =====\n",
      "train_loss: 0.026345318190578837\n",
      "module_EU_LG===== Epoch 20 =====\n",
      "train_loss: 0.027713049559679348\n",
      "module_EU_LG===== Epoch 21 =====\n",
      "train_loss: 0.028822331900300924\n",
      "module_EU_LG===== Epoch 22 =====\n",
      "train_loss: 0.028769816242856905\n",
      "module_EU_LG===== Epoch 23 =====\n",
      "train_loss: 0.027327331277774647\n",
      "module_EU_LG===== Epoch 24 =====\n",
      "train_loss: 0.02468327243695967\n",
      "module_EU_LG===== Epoch 25 =====\n",
      "train_loss: 0.026861565296712797\n",
      "module_EU_LG===== Epoch 26 =====\n",
      "train_loss: 0.028478723579610232\n",
      "module_EU_LG===== Epoch 27 =====\n",
      "train_loss: 0.0317199864439317\n",
      "module_EU_LG===== Epoch 28 =====\n",
      "train_loss: 0.03014528022322338\n",
      "module_EU_LG===== Epoch 29 =====\n",
      "train_loss: 0.0332490431246697\n",
      "module_EU_LG===== Epoch 30 =====\n",
      "train_loss: 0.029264449753100052\n",
      "module_EU_LG===== Epoch 31 =====\n",
      "train_loss: 0.03030008708446985\n",
      "module_EU_LG===== Epoch 32 =====\n",
      "train_loss: 0.023207643134810496\n",
      "module_EU_LG===== Epoch 33 =====\n",
      "train_loss: 0.029297448039869778\n",
      "module_EU_LG===== Epoch 34 =====\n",
      "train_loss: 0.025124876316112932\n",
      "module_EU_LG===== Epoch 35 =====\n",
      "train_loss: 0.026159609857131727\n",
      "module_EU_LG===== Epoch 36 =====\n",
      "train_loss: 0.02716786240489455\n",
      "module_EU_LG===== Epoch 37 =====\n",
      "train_loss: 0.027084745350293815\n",
      "module_EU_LG===== Epoch 38 =====\n",
      "train_loss: 0.03127962195139844\n",
      "module_EU_LG===== Epoch 39 =====\n",
      "train_loss: 0.027203145589737687\n",
      "module_EU_LG===== Epoch 40 =====\n",
      "train_loss: 0.023803357653378043\n",
      "module_EU_LG===== Epoch 41 =====\n",
      "train_loss: 0.02311065800313372\n",
      "module_EU_LG===== Epoch 42 =====\n",
      "train_loss: 0.024682369257789105\n",
      "module_EU_LG===== Epoch 43 =====\n",
      "train_loss: 0.027661017134960275\n",
      "module_EU_LG===== Epoch 44 =====\n",
      "train_loss: 0.021961180937069003\n",
      "module_EU_LG===== Epoch 45 =====\n",
      "train_loss: 0.022133663514978252\n",
      "module_EU_LG===== Epoch 46 =====\n",
      "train_loss: 0.022904640361957718\n",
      "module_EU_LG===== Epoch 47 =====\n",
      "train_loss: 0.06208109070576029\n",
      "module_EU_LG===== Epoch 48 =====\n",
      "train_loss: 0.026550606315140612\n",
      "module_EU_LG===== Epoch 49 =====\n",
      "train_loss: 0.023830151905713137\n",
      "------------------------------------------------------------------\n",
      "5\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_11820/2375363656.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_model.layer_1.bias.data = torch.tensor(a, dtype = torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "-----------\n",
      "-----------\n",
      "-----------\n",
      "=============reorganising===================\n",
      "regularising loss: 126756179.66849792\n",
      "bad\n",
      "module_EU_LG===== Epoch 0 =====\n",
      "train_loss: 26858068.615554273\n",
      "module_EU_LG===== Epoch 1 =====\n",
      "train_loss: 36964291.591430664\n",
      "module_EU_LG===== Epoch 2 =====\n",
      "train_loss: 34360583.56359863\n",
      "module_EU_LG===== Epoch 3 =====\n",
      "train_loss: 65543389.85961914\n",
      "module_EU_LG===== Epoch 4 =====\n",
      "train_loss: 5008602.617767334\n",
      "module_EU_LG===== Epoch 5 =====\n",
      "train_loss: 5624437.182113647\n",
      "module_EU_LG===== Epoch 6 =====\n",
      "train_loss: 1397819.9947052002\n",
      "module_EU_LG===== Epoch 7 =====\n",
      "train_loss: 300878.41504478455\n",
      "module_EU_LG===== Epoch 8 =====\n",
      "train_loss: 206481.84754753113\n",
      "module_EU_LG===== Epoch 9 =====\n",
      "train_loss: 79683.29139709473\n",
      "module_EU_LG===== Epoch 10 =====\n",
      "train_loss: 1002743.5425567627\n",
      "module_EU_LG===== Epoch 11 =====\n",
      "train_loss: 679947.0297737122\n",
      "module_EU_LG===== Epoch 12 =====\n",
      "train_loss: 627091.8047809601\n",
      "module_EU_LG===== Epoch 13 =====\n",
      "train_loss: 1485790.5432052612\n",
      "module_EU_LG===== Epoch 14 =====\n",
      "train_loss: 5143486.236114502\n",
      "module_EU_LG===== Epoch 15 =====\n",
      "train_loss: 165031891.62841797\n",
      "module_EU_LG===== Epoch 16 =====\n",
      "train_loss: 75275619.27636719\n",
      "module_EU_LG===== Epoch 17 =====\n",
      "train_loss: 798063.806892395\n",
      "module_EU_LG===== Epoch 18 =====\n",
      "train_loss: 215754.39693641663\n",
      "module_EU_LG===== Epoch 19 =====\n",
      "train_loss: 397395.45903015137\n",
      "module_EU_LG===== Epoch 20 =====\n",
      "train_loss: 196314.13521575928\n",
      "module_EU_LG===== Epoch 21 =====\n",
      "train_loss: 210925.3699798584\n",
      "module_EU_LG===== Epoch 22 =====\n",
      "train_loss: 45785.71056365967\n",
      "module_EU_LG===== Epoch 23 =====\n",
      "train_loss: 434618.8963699341\n",
      "module_EU_LG===== Epoch 24 =====\n",
      "train_loss: 8445233.42956543\n",
      "module_EU_LG===== Epoch 25 =====\n",
      "train_loss: 9820773.298217773\n",
      "module_EU_LG===== Epoch 26 =====\n",
      "train_loss: 459170.91749954224\n",
      "module_EU_LG===== Epoch 27 =====\n",
      "train_loss: 831952.8601322174\n",
      "module_EU_LG===== Epoch 28 =====\n",
      "train_loss: 6546937.167358398\n",
      "module_EU_LG===== Epoch 29 =====\n",
      "train_loss: 17370958.61717224\n",
      "module_EU_LG===== Epoch 30 =====\n",
      "train_loss: 47690952.81689453\n",
      "module_EU_LG===== Epoch 31 =====\n",
      "train_loss: 84825092.87963867\n",
      "module_EU_LG===== Epoch 32 =====\n",
      "train_loss: 127213110.41113281\n",
      "module_EU_LG===== Epoch 33 =====\n",
      "train_loss: 38071035.53845215\n",
      "module_EU_LG===== Epoch 34 =====\n",
      "train_loss: 3446333.913696289\n",
      "module_EU_LG===== Epoch 35 =====\n",
      "train_loss: 1126349.371673584\n",
      "module_EU_LG===== Epoch 36 =====\n",
      "train_loss: 361681.12046813965\n",
      "module_EU_LG===== Epoch 37 =====\n",
      "train_loss: 255172.05318450928\n",
      "module_EU_LG===== Epoch 38 =====\n",
      "train_loss: 189624.18855285645\n",
      "module_EU_LG===== Epoch 39 =====\n",
      "train_loss: 125312.22318530083\n",
      "module_EU_LG===== Epoch 40 =====\n",
      "train_loss: 1635654.1824951172\n",
      "module_EU_LG===== Epoch 41 =====\n",
      "train_loss: 211496299.99829102\n",
      "module_EU_LG===== Epoch 42 =====\n",
      "train_loss: 7514312.212127686\n",
      "module_EU_LG===== Epoch 43 =====\n",
      "train_loss: 2172235.6454086304\n",
      "module_EU_LG===== Epoch 44 =====\n",
      "train_loss: 758617.7759170532\n",
      "module_EU_LG===== Epoch 45 =====\n",
      "train_loss: 207313.09127044678\n",
      "module_EU_LG===== Epoch 46 =====\n",
      "train_loss: 184642.32077026367\n",
      "module_EU_LG===== Epoch 47 =====\n",
      "train_loss: 489181.3090133667\n",
      "module_EU_LG===== Epoch 48 =====\n",
      "train_loss: 288030.6307182312\n",
      "module_EU_LG===== Epoch 49 =====\n",
      "train_loss: 287598.63573646545\n",
      "=============reorganising===================\n",
      "regularising loss: 12356567.918851614\n",
      "bad\n",
      "module_EU_LG===== Epoch 0 =====\n",
      "train_loss: 44595149.98009431\n",
      "module_EU_LG===== Epoch 1 =====\n",
      "train_loss: 5789041.420135498\n",
      "module_EU_LG===== Epoch 2 =====\n",
      "train_loss: 118018.4835395813\n",
      "module_EU_LG===== Epoch 3 =====\n",
      "train_loss: 99780.86682510376\n",
      "module_EU_LG===== Epoch 4 =====\n",
      "train_loss: 166982.02758026123\n",
      "module_EU_LG===== Epoch 5 =====\n",
      "train_loss: 269103.2469139099\n",
      "module_EU_LG===== Epoch 6 =====\n",
      "train_loss: 124799033.5345993\n",
      "module_EU_LG===== Epoch 7 =====\n",
      "train_loss: 164365433.09817505\n",
      "module_EU_LG===== Epoch 8 =====\n",
      "train_loss: 1325154.095855713\n",
      "module_EU_LG===== Epoch 9 =====\n",
      "train_loss: 392466.339012146\n",
      "module_EU_LG===== Epoch 10 =====\n",
      "train_loss: 147074.47877120972\n",
      "module_EU_LG===== Epoch 11 =====\n",
      "train_loss: 108704.4803276062\n",
      "module_EU_LG===== Epoch 12 =====\n",
      "train_loss: 104513.10727882385\n",
      "module_EU_LG===== Epoch 13 =====\n",
      "train_loss: 68292.91115760803\n",
      "module_EU_LG===== Epoch 14 =====\n",
      "train_loss: 579245.2145385742\n",
      "module_EU_LG===== Epoch 15 =====\n",
      "train_loss: 479179.20358657837\n",
      "module_EU_LG===== Epoch 16 =====\n",
      "train_loss: 274712.76565361023\n",
      "module_EU_LG===== Epoch 17 =====\n",
      "train_loss: 72231.08067131042\n",
      "module_EU_LG===== Epoch 18 =====\n",
      "train_loss: 228913.70336532593\n",
      "module_EU_LG===== Epoch 19 =====\n",
      "train_loss: 187011.3496761322\n",
      "module_EU_LG===== Epoch 20 =====\n",
      "train_loss: 111483.9449558258\n",
      "module_EU_LG===== Epoch 21 =====\n",
      "train_loss: 872331.4931716919\n",
      "module_EU_LG===== Epoch 22 =====\n",
      "train_loss: 9615681.441467285\n",
      "module_EU_LG===== Epoch 23 =====\n",
      "train_loss: 27356810.463134766\n",
      "module_EU_LG===== Epoch 24 =====\n",
      "train_loss: 64833198.721191406\n",
      "module_EU_LG===== Epoch 25 =====\n",
      "train_loss: 17448605.237548828\n",
      "module_EU_LG===== Epoch 26 =====\n",
      "train_loss: 2644538.9032669067\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11820/381727409.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcriterion_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     reorganising_model(model = model, hidden_dim = hidden_dim, epochs_reg = epochs_reg, \n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mepochs_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_reg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mcriterion_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11820/600141083.py\u001b[0m in \u001b[0;36mreorganising_model\u001b[1;34m(model, hidden_dim, epochs_reg, epochs_weight, iterator, optimizer_reg, criterion_reg, lr_reg, lr_weight, epsilon_reg, epsilon_lr, epsilon_weight)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcritireon_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         a = module_weight_EU_LG(new_model, epsilon = epsilon_weight, iterator = iterator, \n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         optimizer = optimizer_weight, criterion  = critireon_weight, epochs = epochs_weight)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11820/439844393.py\u001b[0m in \u001b[0;36mmodule_weight_EU_LG\u001b[1;34m(model, epsilon, iterator, X_train, y_train, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss_individual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_individual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'acceptable'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a, loss_individual = module_weight_EU_LG(model = model, epsilon = epsilon_weight, iterator = train_loader, \n",
    "X_train = X_train, y_train = y_train, optimizer = optimizer_weight, criterion = criterion_weight, \n",
    "epochs = epochs_weight)\n",
    "\n",
    "\n",
    "if a == 'unacceptable':\n",
    "    print('------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    index = []\n",
    "    for i in range(len(loss_individual)):\n",
    "        if loss_individual[i]>0.1:\n",
    "            index.append(i)\n",
    "\n",
    "    print(len(index))\n",
    "\n",
    "    model = TwoLayerNet(50,10,1)\n",
    "    for k in index:\n",
    "        print('-----------')\n",
    "        cramming(model, k, hidden_dim = hidden_dim, X_train = X_train, \n",
    "        y_train=y_train)\n",
    "        model = TwoLayerNet(50,hidden_dim+3,1)\n",
    "        model.load_state_dict(torch.load('PATH'))\n",
    "        hidden_dim+=3\n",
    "    torch.save(model.state_dict(), 'PATH')\n",
    "    \n",
    "    optimizer_reg = optim.Adam(model.parameters(), lr = 0.01)\n",
    "    criterion_reg = torch.nn.MSELoss()\n",
    "\n",
    "    reorganising_model(model = model, hidden_dim = hidden_dim, epochs_reg = epochs_reg, \n",
    "    epochs_weight = epochs_weight, iterator = train_loader, optimizer_reg = optimizer_reg, \n",
    "    criterion_reg = criterion_reg, lr_reg = lr_reg, lr_weight = lr_weight, \n",
    "    epsilon_reg = epochs_reg, epsilon_lr = epsilon_lr, epsilon_weight = epsilon_weight)\n",
    "\n",
    "else:\n",
    "    reorganising_model(model = model, hidden_dim = hidden_dim, epochs_reg = epochs_reg, \n",
    "    epochs_weight = epochs_weight, iterator = train_loader, optimizer_reg = optimizer_reg, \n",
    "    criterion_reg = criterion_reg, lr_reg = lr_reg, lr_weight = lr_weight, \n",
    "    epsilon_reg = epochs_reg, epsilon_lr = epsilon_lr, epsilon_weight = epsilon_weight)\n",
    "\n",
    "test(model = model, iterator = train_loader, criterion = criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
