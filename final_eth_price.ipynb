{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2'\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('Copper_forecasting_data.csv')\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = data.drop([\"y\"], axis = 1)\n",
    "X = sc.fit_transform(X)\n",
    "y = data[\"y\"] / 1000\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "batch_size = 30\n",
    "\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:] \n",
    "\n",
    "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.float32)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        X_, y_ = self.X[index], self.y[index]\n",
    "        return X_, y_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_train.to(device), y_train.to(device)), batch_size = batch_size, shuffle=False, drop_last = False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_test.to(device), y_test.to(device)), batch_size = batch_size, shuffle=False, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.Reorg import *\n",
    "from module.Weight_tune import *\n",
    "\n",
    "# weight tune module\n",
    "hidden_dim = 50\n",
    "model = TwoLayerNet(18, hidden_dim, 1)\n",
    "\n",
    "config_w_tune = {\n",
    "    \"epochs\": 100,\n",
    "    \"epsilon\": 10, # 13 for getting acceptable wt\n",
    "    \"lr_lowerbound\": 1e-6,\n",
    "    \"optimizer\": optim.Adam(model.parameters(), lr = 0.01),\n",
    "    \"criterion\": torch.nn.MSELoss(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module_weight_EU_LG_UA\n",
    "- To acheive a acceptable SLFN\n",
    "- store in acceptable/wt.pth\n",
    "- Already acheived, skip this process, jump to reorganise module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Acceptable: iteratively random initialisation of weight tuning module\n",
    "while True:\n",
    "    acceptable, model, train_loss, test_loss = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_w_tune)\n",
    "    if acceptable == True:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Acceptable\n",
    "acceptable, model, train_loss, test_loss = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_w_tune)\n",
    "eps, pred = eps_for_each(train_loader, model)\n",
    "acceptable, max(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for weight tuning module: 767.6356582641602\n"
     ]
    }
   ],
   "source": [
    "# test loss\n",
    "loss_test = 0\n",
    "criterion = nn.MSELoss()\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = model(X)\n",
    "    loss_test_ = criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "loss_test /= len(test_loader)\n",
    "print(f\"Test loss for weight tuning module: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable SLFN exist in 'acceptable/wt.pth'.\n"
     ]
    }
   ],
   "source": [
    "# check if acceptable SLFN exist\n",
    "import os\n",
    "acceptable_wt_path = 'acceptable/wt.pth'\n",
    "if os.path.exists(acceptable_wt_path):\n",
    "    print(f\"Acceptable SLFN exist in '{acceptable_wt_path}'.\")\n",
    "    model = torch.load(acceptable_wt_path)\n",
    "else:\n",
    "    print(f\"Acceptable SLFN not exist in '{acceptable_wt_path}'.\")\n",
    "    model = None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module reorganising_EU_LG_UA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regularise with Pytorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md)\n",
    "- L7 p 63\n",
    "- Classmate p 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([14.3732]),\n",
       " TwoLayerNet(\n",
       "   (layer_1): Linear(in_features=18, out_features=164, bias=True)\n",
       "   (layer_out): Linear(in_features=164, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_model = torch.load(\"acceptable/Cram.pth\")\n",
    "eps, y_pred = eps_for_each(train_loader, previous_model)\n",
    "max(eps), previous_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.Reorg import *\n",
    "# reorganising module\n",
    "\"\"\"\n",
    "# these config need to consist to config in weight tune module:\n",
    "#   1. hidden_dim\n",
    "#   2. epochs\n",
    "#   3. eps\n",
    "#   4. criterion\n",
    "#   5. lr_bounds\n",
    "# Note: hidden dim should check the previous model \n",
    "\"\"\"\n",
    "config_reorg  = {\n",
    "    \"hidden_dim\": 164,\n",
    "    \"epochs\": 100,\n",
    "    \"criterion\": nn.MSELoss(),     # loss function\n",
    "    \"lr_reg\": 0.01,                # learning rate for regularisation\n",
    "    \"lr_w_tune\": 0.01,             # learning rate for weight tuning\n",
    "    \"lr_bound_reg\": 1e-6,          # lower bound of learning rate regularisation\n",
    "    \"lr_bound_w_tune\": 1e-6,       # lower bound of learning rate weight tuning\n",
    "    \"eps_reg\": 10,               # if regular eps < eps_reg: accept the model\n",
    "    \"eps_w_tune\": 10,            # if weight tune eps < eps_w_tune: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular module\n",
    "reorg = reorganising(\"Cram\", train_loader, test_loader, **config_reorg)\n",
    "reorg.reorganising()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612.7219390869141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = reorg.model(X)\n",
    "    loss_test_ = reorg.criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "loss_test /= len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ========= How to use weight tune, regular, and trim =========\\n# Weight tune ----------------------------------\\nmodel = TwoLayerNet(9, config[\"hidden_dim\"], 1)\\noptimizer = optim.SGD(model.parameters(), lr=config[\"lr_w_tune\"])\\nreorg.module_weight_EU_LG_UA(model, optimizer)\\n# Regular --------------------------------------\\nacceptable, model, train_loss, test_loss = reorg.regularising_EU_LG_UA()\\n# Trim model --------------------------------------\\nmodel_ = reorg.trim_model_nodes(2)\\nprint(reorg.model, model_)\\n# Reorg ----------------------------------------------\\nreorg.reorganising()\\n# final model ------------------------------------------\\nreorg.model\\n# test final model -------------------------------------\\nloss_test = 0\\nfor _, (X, y) in enumerate(test_loader):\\n    pred_test = reorg.model(X)\\n    loss_test_ = reorg.criterion(pred_test, y)\\n    loss_test += loss_test_.item()\\nprint(loss_test)\\n# load reorganise trained model----------------------------------\\nmodel_reorg = torch.load(\"final_model/Reorg\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ========= How to use weight tune, regular, and trim =========\n",
    "# Weight tune ----------------------------------\n",
    "model = TwoLayerNet(9, config[\"hidden_dim\"], 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=config[\"lr_w_tune\"])\n",
    "reorg.module_weight_EU_LG_UA(model, optimizer)\n",
    "# Regular --------------------------------------\n",
    "acceptable, model, train_loss, test_loss = reorg.regularising_EU_LG_UA()\n",
    "# Trim model --------------------------------------\n",
    "model_ = reorg.trim_model_nodes(2)\n",
    "print(reorg.model, model_)\n",
    "# Reorg ----------------------------------------------\n",
    "reorg.reorganising()\n",
    "# final model ------------------------------------------\n",
    "reorg.model\n",
    "# test final model -------------------------------------\n",
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = reorg.model(X)\n",
    "    loss_test_ = reorg.criterion(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "print(loss_test)\n",
    "# load reorganise trained model----------------------------------\n",
    "model_reorg = torch.load(\"final_model/Reorg\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (layer_1): Linear(in_features=9, out_features=28, bias=True)\n",
       "  (layer_out): Linear(in_features=28, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reorg = torch.load(\"acceptable/Reorg.pth\")\n",
    "model_reorg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module_ReLU_RI_SO_RE_MU: Cramming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: L9, p5 | L8, p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cramming(nn.Module):\n",
    "    def __init__(self, train_loader, X_train, y_train, eps_bound, s):\n",
    "        \n",
    "        \"\"\"\n",
    "        model: \n",
    "        train_loader\n",
    "        X_train:\n",
    "        y_train:\n",
    "        eps_bound: maximum epsilon accepted for EACH sample\n",
    "                   as epsilon in weight tuning module and eps_reg, eps_w_tune in module above\n",
    "        s: float (tiny) that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        super(cramming, self).__init__()\n",
    "        if os.path.exists(\"unacceptable/wt.pth\"):\n",
    "            self.model = torch.load(\"unacceptable/wt.pth\")\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(f\"The unacceptable model does not exist.\")\n",
    "        self.input_dim = self.model.layer_1.weight.data.shape[1]\n",
    "        self.hidden_dim = self.model.layer_1.weight.data.shape[0]\n",
    "        self.train_loader = train_loader\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.eps_bound = eps_bound\n",
    "        (\n",
    "        self.eps,\n",
    "        self.y_pred\n",
    "        ) = eps_for_each(train_loader, self.model)\n",
    "        self.s = s\n",
    "        self.ks = torch.where(self.eps > self.eps_bound)[0]\n",
    "    \n",
    "\n",
    "    def cram(self):\n",
    "        for i, k in enumerate(self.ks):\n",
    "            print(f\"cramming sample {k} |{i/len(self.ks)*100:.2f}% total of {len(self.ks)}\")\n",
    "            r = self.cram_find_r(k)\n",
    "            self.cram_add_node(r, k)\n",
    "        torch.save(self.model, \"acceptable/Cram.pth\")\n",
    "\n",
    "    def cram_find_r(self, k): \n",
    "        # L9, isolation R2: p39, carm: p.54, for multiple case: p.60?\n",
    "        \"\"\"\n",
    "        k: k sample to cram (unaccepted sample with too large epsilon)\n",
    "        ==========\n",
    "        outputs\n",
    "        r: vector that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        print(\"find r: r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\", end = \"\\r\")\n",
    "\n",
    "        Xc_Xk = []\n",
    "\n",
    "        k = 1\n",
    "        X_no_k = torch.cat([X_train[:k], X_train[k+1:]], dim = 0)\n",
    "        if torch.any(torch.all(X_no_k == X_train[k], dim=1)):\n",
    "            print(\"If X_train[k] in X_train: yes. check again\")\n",
    "        \n",
    "        n = 0\n",
    "        while True:\n",
    "            n+=1\n",
    "            print(f\"try vector {n}\", end=\"\\r\")\n",
    "\n",
    "            r = torch.rand(self.input_dim)            \n",
    "            dots = ((X_no_k - X_train[k]) @ r.T) \n",
    "            print(max((self.s + dots) * (self.s - dots)))\n",
    "            \n",
    "            if (torch.sum(dots == 0) == 0) and (max((self.s + dots) * (self.s - dots)) < 0):\n",
    "                return r\n",
    "            \n",
    "    def cram_add_node(self, r, k):\n",
    "        \"\"\"\n",
    "        k: k sample to cram (unaccepted sample with too large epsilon)\n",
    "        r: vector that r*(Xc - Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
    "        \"\"\"\n",
    "        new_model = TwoLayerNet(self.input_dim, self.hidden_dim+3, 1)\n",
    "\n",
    "        param = self.model.state_dict()\n",
    "        for name in model.state_dict():\n",
    "            if name == 'layer_1.weight':\n",
    "                # First node\n",
    "                new_w = torch.cat([param[name], r.reshape(1, -1)], dim = 0)\n",
    "                # Second node\n",
    "                new_w = torch.cat([new_w, r.reshape(1, -1)], dim = 0)\n",
    "                # Third node\n",
    "                new_w = torch.cat([new_w, r.reshape(1, -1)], dim = 0)\n",
    "\n",
    "                new_model.layer_1.weight.data = new_w\n",
    "                    \n",
    "            if name == 'layer_1.bias':\n",
    "                # First node\n",
    "                node_add = self.s - torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([param[name], node_add.reshape(1)], dim = 0)\n",
    "                # Second node\n",
    "                node_add = (-1) * torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([new_b, node_add.reshape(1)], dim = 0)\n",
    "                # Third node\n",
    "                node_add = (-1)*self.s - torch.dot(r, self.X_train[k])\n",
    "                new_b = torch.cat([new_b, node_add.reshape(1)], dim = 0)\n",
    "\n",
    "                new_model.layer_1.bias.data = new_b\n",
    "\n",
    "            if name == 'layer_out.weight':\n",
    "                \"\"\"\n",
    "                # the base of Xk = (yk - prediction yk)/s\n",
    "                aik = nn.ReLU()(model.layer_1.weight.data @ X_train[k].T)\n",
    "                out_weight = model.layer_out.weight.data.reshape(-1)\n",
    "                out_bias = model.layer_out.bias.data.reshape(-1)\n",
    "                base = (y_train[k] - out_bias - torch.dot(out_weight, aik))/s\n",
    "                \"\"\"\n",
    "                base = (self.y_train[k] - self.model(self.X_train[k]))/self.s\n",
    "                # First node\n",
    "                new_w_o = torch.cat([param[name], base.reshape(1, 1)], dim = 1)\n",
    "                # Second node\n",
    "                new_w_o = torch.cat([new_w_o, ((-2)*base).reshape(1, 1)], dim = 1)\n",
    "                # Third node\n",
    "                new_w_o = torch.cat([new_w_o, base.reshape(1, 1)], dim = 1)\n",
    "\n",
    "                new_model.layer_out.weight.data = new_w_o\n",
    "\n",
    "        self.model = new_model\n",
    "        self.hidden_dim +=3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"unacceptable/wt.pth\")\n",
    "eps, pred = eps_for_each(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cramming sample 4 |0.00% total of 38\n",
      "tensor(0.0098)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0273)\n",
      "cramming sample 5 |2.63% total of 38\n",
      "tensor(0.0063)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0115)\n",
      "cramming sample 6 |5.26% total of 38\n",
      "tensor(0.0092)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0099)\n",
      "tensor(0.0047)\n",
      "tensor(-0.0048)\n",
      "cramming sample 7 |7.89% total of 38\n",
      "tensor(-0.0014) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 8 |10.53% total of 38\n",
      "tensor(0.0100)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0094)\n",
      "tensor(0.0043)\n",
      "tensor(0.0014)\n",
      "tensor(0.0094)\n",
      "tensor(-0.0052)\n",
      "cramming sample 29 |13.16% total of 38\n",
      "tensor(-0.1254) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 30 |15.79% total of 38\n",
      "tensor(0.0098)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0098)\n",
      "tensor(0.0061)\n",
      "tensor(0.0090)\n",
      "tensor(0.0035)\n",
      "tensor(0.0077)\n",
      "tensor(0.0097)\n",
      "tensor(-0.0154)\n",
      "cramming sample 31 |18.42% total of 38\n",
      "tensor(0.0055)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0098)\n",
      "tensor(0.0098)\n",
      "tensor(0.0090)\n",
      "tensor(0.0039)\n",
      "tensor(-0.0267)\n",
      "cramming sample 32 |21.05% total of 38\n",
      "tensor(-0.0028) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 33 |23.68% total of 38\n",
      "tensor(0.0089)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0939)\n",
      "cramming sample 34 |26.32% total of 38\n",
      "tensor(0.0088)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0042)\n",
      "cramming sample 36 |28.95% total of 38\n",
      "tensor(0.0070)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0008)\n",
      "cramming sample 38 |31.58% total of 38\n",
      "tensor(0.0021)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0018)\n",
      "tensor(-0.0003)\n",
      "cramming sample 39 |34.21% total of 38\n",
      "tensor(-0.0238) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 40 |36.84% total of 38\n",
      "tensor(-0.0215) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 41 |39.47% total of 38\n",
      "tensor(0.0100)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0093)\n",
      "tensor(-0.0502)\n",
      "cramming sample 54 |42.11% total of 38\n",
      "tensor(-0.0077) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 204 |44.74% total of 38\n",
      "tensor(0.0079)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0084)\n",
      "tensor(0.0100)\n",
      "tensor(0.0074)\n",
      "tensor(-0.0063)\n",
      "cramming sample 205 |47.37% total of 38\n",
      "tensor(-0.0043) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 206 |50.00% total of 38\n",
      "tensor(0.0098)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0100)\n",
      "tensor(0.0092)\n",
      "tensor(-0.0285)\n",
      "cramming sample 207 |52.63% total of 38\n",
      "tensor(0.0075)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0092)\n",
      "tensor(-0.0010)\n",
      "cramming sample 208 |55.26% total of 38\n",
      "tensor(0.0071)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0239)\n",
      "cramming sample 209 |57.89% total of 38\n",
      "tensor(0.0069)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0093)\n",
      "cramming sample 210 |60.53% total of 38\n",
      "tensor(0.0059)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0094)\n",
      "tensor(-0.0047)\n",
      "cramming sample 211 |63.16% total of 38\n",
      "tensor(0.0057)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0098)\n",
      "tensor(0.0100)\n",
      "tensor(0.0075)\n",
      "tensor(-0.0185)\n",
      "cramming sample 212 |65.79% total of 38\n",
      "tensor(0.0091)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0076)\n",
      "tensor(0.0086)\n",
      "tensor(0.0031)\n",
      "tensor(-0.0392)\n",
      "cramming sample 213 |68.42% total of 38\n",
      "tensor(0.0022)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0061)\n",
      "tensor(-0.0113)\n",
      "cramming sample 214 |71.05% total of 38\n",
      "tensor(-0.1154) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 215 |73.68% total of 38\n",
      "tensor(-0.0001) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 216 |76.32% total of 38\n",
      "tensor(0.0096)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0431)\n",
      "cramming sample 217 |78.95% total of 38\n",
      "tensor(0.0098)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0097)\n",
      "tensor(0.0030)\n",
      "tensor(-0.0032)\n",
      "cramming sample 218 |81.58% total of 38\n",
      "tensor(-0.0558) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 219 |84.21% total of 38\n",
      "tensor(0.0099)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0091)\n",
      "tensor(-0.0116)\n",
      "cramming sample 249 |86.84% total of 38\n",
      "tensor(0.0041)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0408)\n",
      "cramming sample 250 |89.47% total of 38\n",
      "tensor(-0.0265) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 251 |92.11% total of 38\n",
      "tensor(-0.0447) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 252 |94.74% total of 38\n",
      "tensor(-0.0364) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "cramming sample 305 |97.37% total of 38\n",
      "tensor(-0.0002) Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (layer_1): Linear(in_features=18, out_features=164, bias=True)\n",
       "  (layer_out): Linear(in_features=164, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model, vs cram.model\n",
    "model = torch.load(\"unacceptable/wt.pth\")\n",
    "config_cramming = {\n",
    "    \"eps_bound\": np.percentile(eps.detach().numpy(), 90), \n",
    "    \"s\": 0.1,\n",
    "}\n",
    "cram = cramming(train_loader, X_train, y_train, **config_cramming)\n",
    "cram.cram()\n",
    "cram.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679.9348983764648\n"
     ]
    }
   ],
   "source": [
    "loss_test = 0\n",
    "for _, (X, y) in enumerate(test_loader):\n",
    "    pred_test = cram.model(X)\n",
    "    loss_test_ = nn.MSELoss()(pred_test, y)\n",
    "    loss_test += loss_test_.item()\n",
    "loss_test /= len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0073,  0.0435,  0.0546, -0.2061, -0.1659,  0.1690, -0.1748,  0.1323,\n",
       "          -0.1250, -0.1714,  0.0733, -0.0514,  0.0334,  0.1305, -0.0112,  0.1752,\n",
       "          -0.0449,  0.0449, -0.0729, -0.1455]]),\n",
       " tensor([[ 7.2531e-03,  4.3481e-02,  5.4579e-02, -2.0614e-01, -1.6595e-01,\n",
       "           1.6896e-01, -1.7480e-01,  1.3230e-01, -1.2500e-01, -1.7137e-01,\n",
       "           7.3287e-02, -5.1439e-02,  3.3421e-02,  1.3048e-01, -1.1244e-02,\n",
       "           1.7522e-01, -4.4895e-02,  4.4885e-02, -7.2886e-02, -1.4554e-01,\n",
       "           2.1068e+04, -4.2136e+04,  2.1068e+04,  1.0528e+05, -2.1057e+05,\n",
       "           1.0528e+05,  1.9568e+04, -3.9137e+04,  1.9568e+04, -1.7484e+04,\n",
       "           3.4968e+04, -1.7484e+04, -4.7019e+04,  9.4038e+04, -4.7019e+04,\n",
       "           1.1945e+04, -2.3890e+04,  1.1945e+04]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights\n",
    "model.layer_1.weight.data, new_model.layer_1.weight.data.shape\n",
    "model.layer_1.bias.data, new_model.layer_1.bias.data\n",
    "model.layer_out.weight.data, new_model.layer_out.weight.data\n",
    "# check grad\n",
    "# model.layer_1.weight.requires_grad, new_model.layer_1.weight.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
