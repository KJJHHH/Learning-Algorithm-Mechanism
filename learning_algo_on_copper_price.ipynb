{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from module.model import TwoLayerNet\n",
    "from module.Weight_tune import *\n",
    "from module.Reorg import *\n",
    "from module.Cram import *\n",
    "from module.init import *\n",
    "from module.lts import *\n",
    "from module.utils import *\n",
    "from module.data import *\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64\n",
    "\n",
    "data = pd.read_csv('Copper_forecasting_data.csv')\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = data.drop([\"y\"], axis = 1)\n",
    "X = sc.fit_transform(X)\n",
    "y = data[\"y\"] / 1000\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "batch_size = 30\n",
    "\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:] \n",
    "\n",
    "X_train = torch.tensor(np.array(X_train), dtype=dtype)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=dtype)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=dtype)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=dtype)\n",
    "input_dim = X_train.shape[1]\n",
    "    \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_test.to(device), y_test.to(device)), \n",
    "    batch_size = batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning goal: max eps < learning goal\n",
    "learning_goal = torch.exp(torch.tensor(1)).to(dtype = dtype)\n",
    "lr_rate = .01\n",
    "lr_bound = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "\"\"\"\n",
    "# Note\n",
    "1. hidden dim should check the previous model. dynamically change\n",
    "2. For lr_rate, lr bound, lr goal (eps bound) are all the same fro eahc module\n",
    "\"\"\"\n",
    "config_wt = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "}\n",
    "\n",
    "config_cram = {\n",
    "    \"lr_goal\": learning_goal, \n",
    "    \"s\": 0.001,                     # a small num in cram\n",
    "}\n",
    "\n",
    "config_reorg  = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}\n",
    "# NOTE \n",
    "# 1. for leaning goals, if first using weightune and no LTS or otherthings, \n",
    "# 13 for getting acceptable wt | 10 not acceptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L11 p9, third learning mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the random of init. Should not be random in init.\n",
    "k = 0\n",
    "loss = []\n",
    "while k<2:\n",
    "    model = init_model(X_train, y_train)\n",
    "    # 2. obtaining_LTS / selecting_LTS\n",
    "    train_loader, indices, n = lts(model, X_train, y_train, learning_goal)\n",
    "\n",
    "    # 3. check learning goal\n",
    "    acceptable, model, train_loss_list, test_loss_list = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_wt)\n",
    "    acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "    loss.append(train_loss_list)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result file\n",
    "file_create_time = str(datetime.datetime.now().date())\n",
    "out_file = open(f\"result/{file_create_time}\" + '.txt', 'a')\n",
    "out_file.write(f\"################################## New lts #######################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total obtaining n: 328\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 333\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=1, bias=True)\n",
      "  (layer_out): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[6.9074],\n",
      "        [7.2064],\n",
      "        [7.2828],\n",
      "        [7.3420],\n",
      "        [7.3804],\n",
      "        [7.5212],\n",
      "        [7.6059],\n",
      "        [7.6143],\n",
      "        [7.6658],\n",
      "        [7.6844]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [1.4996e-23],\n",
      "        [1.6117e-23],\n",
      "        [1.1803e-22],\n",
      "        [8.6641e-25],\n",
      "        [8.5323e-25]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 333\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 338\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[6.9074],\n",
      "        [7.2064],\n",
      "        [7.2828],\n",
      "        [7.3420],\n",
      "        [7.3804],\n",
      "        [7.7657],\n",
      "        [7.7761],\n",
      "        [8.0642],\n",
      "        [8.0938],\n",
      "        [8.2421]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=31, bias=True)\n",
      "  (layer_out): Linear(in_features=31, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [3.2572e-24],\n",
      "        [1.0252e-22],\n",
      "        [8.1302e-23],\n",
      "        [3.2465e-21],\n",
      "        [2.4037e-25]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=31, bias=True)\n",
      "  (layer_out): Linear(in_features=31, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 338\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 343\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=31, bias=True)\n",
      "  (layer_out): Linear(in_features=31, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[6.9074],\n",
      "        [7.2064],\n",
      "        [7.2828],\n",
      "        [7.3420],\n",
      "        [7.3804],\n",
      "        [8.4559],\n",
      "        [8.5033],\n",
      "        [8.5618],\n",
      "        [8.9763],\n",
      "        [9.0487]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=46, bias=True)\n",
      "  (layer_out): Linear(in_features=46, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [6.3557e-23],\n",
      "        [1.0469e-24],\n",
      "        [3.1032e-23],\n",
      "        [5.6300e-23],\n",
      "        [2.6314e-22]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=46, bias=True)\n",
      "  (layer_out): Linear(in_features=46, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 343\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 348\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=46, bias=True)\n",
      "  (layer_out): Linear(in_features=46, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[6.9074],\n",
      "        [7.2064],\n",
      "        [7.2828],\n",
      "        [7.3420],\n",
      "        [7.3804],\n",
      "        [9.1321],\n",
      "        [9.2689],\n",
      "        [9.3818],\n",
      "        [9.4262],\n",
      "        [9.5003]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=61, bias=True)\n",
      "  (layer_out): Linear(in_features=61, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [2.8119e-24],\n",
      "        [1.6285e-22],\n",
      "        [1.3704e-23],\n",
      "        [7.5268e-23],\n",
      "        [1.9785e-23]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=61, bias=True)\n",
      "  (layer_out): Linear(in_features=61, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 348\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 353\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=61, bias=True)\n",
      "  (layer_out): Linear(in_features=61, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[ 6.9074],\n",
      "        [ 7.2064],\n",
      "        [ 7.2828],\n",
      "        [ 7.3420],\n",
      "        [ 7.3804],\n",
      "        [ 9.7055],\n",
      "        [10.2107],\n",
      "        [10.2807],\n",
      "        [10.5268],\n",
      "        [10.7969]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=76, bias=True)\n",
      "  (layer_out): Linear(in_features=76, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [3.9722e-23],\n",
      "        [5.8236e-23],\n",
      "        [8.4166e-22],\n",
      "        [1.6180e-21],\n",
      "        [6.8297e-22]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=76, bias=True)\n",
      "  (layer_out): Linear(in_features=76, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 353\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 358\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=76, bias=True)\n",
      "  (layer_out): Linear(in_features=76, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[ 6.9074],\n",
      "        [ 7.2064],\n",
      "        [ 7.2828],\n",
      "        [ 7.3420],\n",
      "        [ 7.3804],\n",
      "        [10.8709],\n",
      "        [11.3433],\n",
      "        [11.4646],\n",
      "        [12.4356],\n",
      "        [12.6703]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=91, bias=True)\n",
      "  (layer_out): Linear(in_features=91, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [1.3414e-24],\n",
      "        [7.8414e-22],\n",
      "        [1.3579e-24],\n",
      "        [5.0766e-22],\n",
      "        [8.2362e-22]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after reorg: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=91, bias=True)\n",
      "  (layer_out): Linear(in_features=91, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total obtaining n: 358\n",
      "obtaining n over lr goal: 0\n",
      "Total select n: 363\n",
      "select n over lr goal: 5\n",
      "//////////// Start module_EU_LG Epoch ///////////\n",
      "////////// Start CRAM and REORG with unacceptable wt //////////\n",
      "model after wt: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=91, bias=True)\n",
      "  (layer_out): Linear(in_features=91, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_square (last 10) before cram: tensor([[ 6.9074],\n",
      "        [ 7.2064],\n",
      "        [ 7.2828],\n",
      "        [ 7.3420],\n",
      "        [ 7.3804],\n",
      "        [12.9719],\n",
      "        [13.5932],\n",
      "        [13.8395],\n",
      "        [13.9810],\n",
      "        [14.2186]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n",
      "model after cram: TwoLayerNet(\n",
      "  (layer_1): Linear(in_features=18, out_features=106, bias=True)\n",
      "  (layer_out): Linear(in_features=106, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "eps_sqaure (last 10) after cram: tensor([[6.9074e+00],\n",
      "        [7.2064e+00],\n",
      "        [7.2828e+00],\n",
      "        [7.3420e+00],\n",
      "        [7.3804e+00],\n",
      "        [1.1778e-23],\n",
      "        [8.6959e-21],\n",
      "        [6.7630e-24],\n",
      "        [5.2836e-23],\n",
      "        [2.8022e-23]], dtype=torch.float64, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Full step for the learning algorithm mechanism\n",
    "# NOTE\n",
    "# 1. model right before reorg always need to be acceptable model\n",
    "# 2. model after cram and reorg always need to be acceptable \n",
    "# 3. check for the above every time after cram and reorg\n",
    "# 4. the randomness: in cram find r\n",
    "#################################################################\n",
    "\n",
    "# 1. initializing_1_ReLU_LR | L11 p2\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(str(datetime.datetime.now()))\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "model = init_model(X_train, y_train)\n",
    "n = 0\n",
    "n_not_fit = 5\n",
    "model_history = {}\n",
    "\n",
    "\n",
    "while n < len(X_train):\n",
    "    out_file.write(f\"### New lts ###\\n\")\n",
    "    models_within_iter = {}\n",
    "    # 2. obtaining_LTS / selecting_LTS\n",
    "    train_loader, indices, X_train_lts, y_train_lts, n = \\\n",
    "        lts(model, X_train, y_train, learning_goal, n_not_fit, out_file)\n",
    "\n",
    "    # 3. check learning goal\n",
    "    acceptable, eps_sqaure, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "    models_within_iter[\"begin\"] = model\n",
    "    if acceptable:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        torch.save(model, \"unacceptable/selecting.pth\")\n",
    "\n",
    "        # store model in acceptable/wt.pth if acceptable\n",
    "        # store model in unacceptable/wt.pth if not acceptable\n",
    "        out_file.write(f\"//////////// Start module_EU_LG_UA Epoch ///////////\\n\")\n",
    "        print(f\"//////////// Start module_EU_LG_UA Epoch ///////////\")\n",
    "        acceptable, model, train_loss_list, test_loss_list = \\\n",
    "            module_weight_EU_LG_UA(model, train_loader, test_loader, out_file, **config_wt)\n",
    "        models_within_iter[\"wt\"] = model\n",
    "        \n",
    "        if acceptable:\n",
    "            out_file.write(\"////////// Start REORG with accpetable wt //////////\\n\")   \n",
    "            out_file.write(f\"model after wt: {model}\\n\")\n",
    "            print(\"////////// Start REORG with accpetable wt //////////\")   \n",
    "            print(f\"model after wt: {model}\")\n",
    "            \n",
    "            # load model in acceptable/wt.pth if wt acceptable\n",
    "            pre_module = \"wt\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "            models_within_iter[\"reorg\"] = model\n",
    "            out_file.write(f\"model after reorg: {model}\\n\")\n",
    "            print(f\"model after reorg: {model}\")\n",
    "\n",
    "        else:\n",
    "            out_file.write(\"////////// Start CRAM and REORG with unacceptable wt //////////\\n\")\n",
    "            print(\"////////// Start CRAM and REORG with unacceptable wt //////////\")\n",
    "\n",
    "            # load model before wt: unacceptable/selecting.pth if unacceptable after wt\n",
    "            model = torch.load(\"unacceptable/selecting.pth\")    \n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            out_file.write(f\"model after wt: {model}\\n\")\n",
    "            out_file.write(f\"eps_square (last 10) before cram: {eps_square[-10:]}\\n\")\n",
    "            print(f\"model after wt: {model}\")\n",
    "            print(f\"eps_square (last 10) before cram: {eps_square[-10:]}\")\n",
    "\n",
    "            # load model in unacceptable/wt.pth if wt not acceptable       \n",
    "            # store acceptable cram in acceptable/cram.pth \n",
    "            cram = cramming(model, X_train[indices], y_train[indices], out_file, **config_cram)            \n",
    "            cram.cram() \n",
    "            model = cram.model\n",
    "            models_within_iter[\"cram\"] = model\n",
    "            out_file.write(f\"model after cram: {model}\\n\")  \n",
    "            print(f\"model after cram: {model}\")      \n",
    "            # check accetable\n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            out_file.write(f\"eps_sqaure (last 10) after cram: {eps_square[-10:]}\\n\")\n",
    "            print(f\"eps_sqaure (last 10) after cram: {eps_square[-10:]}\")\n",
    "            if not acceptable:\n",
    "                out_file.write(\"weird cram\\n\")\n",
    "                print(\"weird cram\")\n",
    "                break\n",
    "\n",
    "            # load model in acceptable/cram.pth if cram acceptable\n",
    "            # store acceptable cram in acceptable/cram.pth\n",
    "            pre_module = \"cram\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "            models_within_iter[\"reorg\"] = model\n",
    "            out_file.write(f\"model after reorg: {model}\\n\")\n",
    "            print(f\"model after reorg: {model}\")\n",
    "\n",
    "            # check \n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            if acceptable:\n",
    "                pass\n",
    "            else:\n",
    "                out_file.write(\"weird reorg\\n\")\n",
    "                print(\"weird reorg\")\n",
    "                break\n",
    "            \n",
    "    model_history[n] = models_within_iter\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6381389892536742, 24.44209036595635)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_loss\n",
    "train_loss = 0\n",
    "for x, y in train_loader:\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    train_loss += loss.item()\n",
    "train_loss/=len(train_loader)\n",
    "\n",
    "test_loss = 0\n",
    "for x, y in test_loader:\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    test_loss += loss.item()\n",
    "test_loss/=len(test_loader)\n",
    "\n",
    "train_loss, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
