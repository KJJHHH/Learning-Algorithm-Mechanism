{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, os\n",
    "from module.model import TwoLayerNet\n",
    "from module.Weight_tune import *\n",
    "from module.Reorg import *\n",
    "from utils import validate, check_acceptable\n",
    "from module.Cram import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('Copper_forecasting_data.csv')\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = data.drop([\"y\"], axis = 1)\n",
    "X = sc.fit_transform(X)\n",
    "y = data[\"y\"] / 1000\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "batch_size = 30\n",
    "\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:] \n",
    "\n",
    "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.float32)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_, y_ = self.X[index], self.y[index]\n",
    "        return X_, y_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_test.to(device), y_test.to(device)), \n",
    "    batch_size = batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning goal: max eps < learning goal\n",
    "learning_goal = torch.sqrt(torch.tensor(.09, dtype=torch.float32)) \n",
    "lr_rate = .01\n",
    "lr_bound = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 100\n",
    "\"\"\"\n",
    "# Note\n",
    "1. hidden dim should check the previous model. dynamically change\n",
    "2. For lr_rate, lr bound, lr goal (eps bound) are all the same fro eahc module\n",
    "\"\"\"\n",
    "config_wt = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "}\n",
    "\n",
    "config_cram = {\n",
    "    \"lr_goal\": learning_goal, \n",
    "    \"s\": 0.01,                     # a small num in cram\n",
    "}\n",
    "\n",
    "config_reorg  = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}\n",
    "# NOTE \n",
    "# 1. for leaning goals, if first using weightune and no LTS or otherthings, \n",
    "# 13 for getting acceptable wt | 10 not acceptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L11 p9, third learning mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total obtaining n: 64\n",
      "obtaining n over lr goal: 0\n",
      "tensor([  9, 330, 352, 110, 290, 302, 323, 319,  85, 214, 192, 271, 325, 332,\n",
      "        372, 236, 324, 322, 333, 102, 199,  92, 114, 139, 131, 358,  73, 112,\n",
      "        288, 137, 357, 212, 148, 165, 184, 237,  75,  97, 362, 159, 101, 268,\n",
      "         33,  16, 318, 147, 227, 141, 142, 127,  98, 215, 373,  19, 218, 138,\n",
      "        155, 213, 361, 111,   3, 140,  11,  21,  44])\n",
      "Total select n: 65\n",
      "select n over lr goal: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x20ba1f56e30>, [tensor(44)], 65)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. initializing_1_ReLU_LR | L11 p2\n",
    "# Get weight by ols and set weight of two layers by the weight\n",
    "def init_model(X_train, y_train):\n",
    "    miny = min(y_train)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, (y_train - miny))\n",
    "    w = torch.tensor(model.coef_, dtype=torch.float32).reshape(1, -1)\n",
    "    b = torch.tensor(model.intercept_, dtype=torch.float32).reshape(1)\n",
    "\n",
    "    model = TwoLayerNet(X_train.shape[1], 1, 1)\n",
    "    model.layer_1.weight.data = w\n",
    "    model.layer_1.bias.data = b\n",
    "    model.layer_out.weight.data = torch.tensor(1, dtype=torch.float32).reshape(1, 1)\n",
    "    model.layer_out.bias.data = miny.reshape(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 2. obtaining_LTS / selecting_LTS\n",
    "def lts(model, X_train, y_train, lr_goal):\n",
    "    \"\"\"\n",
    "    X_train, y_trian, lr_goal\n",
    "    ---\n",
    "    # output:\n",
    "    train_loader: with lts\n",
    "    n of train size \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # predict and residuals\n",
    "    y_pred = model(X_train)\n",
    "    y_train = y_train.reshape(-1 ,1)\n",
    "    resid_square = torch.square(y_pred - y_train).reshape(-1)\n",
    "\n",
    "    # obtaining\n",
    "    # prompt: find the indices of tensor < k only shape 1 tensor\n",
    "    resid_square, sorted_indices = torch.sort(resid_square) # default ascending\n",
    "    indices_lts = sorted_indices[resid_square < lr_goal**2]\n",
    "    X_train_lts, y_train_lts = X_train[indices_lts], y_train[indices_lts]\n",
    "\n",
    "    # check if obtaining is true. 0 is correct\n",
    "    print(f\"Total obtaining n: {len(indices_lts)}\")\n",
    "    print(f\"obtaining n over lr goal: {(torch.square(model(X_train_lts) - y_train_lts) > lr_goal**2).sum()}\")\n",
    "\n",
    "    # selecting\n",
    "    n = len(indices_lts) + 1\n",
    "    indices_lts = sorted_indices[:n]\n",
    "    X_train_lts, y_train_lts = X_train[indices_lts], y_train[indices_lts]\n",
    "\n",
    "    # check if the selected is true. 1 is correct\n",
    "    print(f\"Total select n: {len(indices_lts)}\")\n",
    "    print(f\"select n over lr goal: {(torch.square(model(X_train_lts) - y_train_lts)>lr_goal**2).sum()}\")\n",
    "\n",
    "    \n",
    "    # to tensor \n",
    "    X_train_lts = torch.tensor(np.array(X_train_lts), dtype=torch.float32)\n",
    "    y_train_lts = torch.tensor(np.array(y_train_lts), dtype=torch.float32)\n",
    "    batch_size = 30\n",
    "\n",
    "    # train loader of lts\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        MyDataset(X_train_lts.to(device), y_train_lts.to(device)), \n",
    "        batch_size = batch_size, \n",
    "        shuffle=False, \n",
    "        drop_last = False)\n",
    "\n",
    "    return train_loader, [indices_lts[-1]], len(X_train_lts)\n",
    "\n",
    "model = init_model(X_train, y_train)\n",
    "lts(model, X_train, y_train, learning_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total obtaining n: 64\n",
      "obtaining n over lr goal: 0\n",
      "tensor([  9, 330, 352, 110, 290, 302, 323, 319,  85, 214, 192, 271, 325, 332,\n",
      "        372, 236, 324, 322, 333, 102, 199,  92, 114, 139, 131, 358,  73, 112,\n",
      "        288, 137, 357, 212, 148, 165, 184, 237,  75,  97, 362, 159, 101, 268,\n",
      "         33,  16, 318, 147, 227, 141, 142, 127,  98, 215, 373,  19, 218, 138,\n",
      "        155, 213, 361, 111,   3, 140,  11,  21,  44])\n",
      "Total select n: 65\n",
      "select n over lr goal: 1\n",
      "--------- module_EU_LG Epoch 0 ---------\n",
      "train_loss: 0.0560337562734882\n",
      "Save model and lr increase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\learningalgo\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- module_EU_LG Epoch 1 ---------\n",
      "train_loss: 0.10778767118851344\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 2 ---------\n",
      "train_loss: 0.05327459673086802\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 3 ---------\n",
      "train_loss: 0.08849327452480793\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 4 ---------\n",
      "train_loss: 0.042767499263087906\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 5 ---------\n",
      "train_loss: 0.06665547750890255\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 6 ---------\n",
      "train_loss: 0.034844087436795235\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 7 ---------\n",
      "train_loss: 0.037094175505141415\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 8 ---------\n",
      "train_loss: 0.03143196149418751\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 9 ---------\n",
      "train_loss: 0.03047274798154831\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 10 ---------\n",
      "train_loss: 0.029535545967519283\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 11 ---------\n",
      "train_loss: 0.028810291240612667\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 12 ---------\n",
      "train_loss: 0.02766312037905057\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 13 ---------\n",
      "train_loss: 0.027195720622936886\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 14 ---------\n",
      "train_loss: 0.02679162596662839\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 15 ---------\n",
      "train_loss: 0.026218064439793427\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 16 ---------\n",
      "train_loss: 0.026577993606527645\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 17 ---------\n",
      "train_loss: 0.027604201808571815\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 18 ---------\n",
      "train_loss: 0.0294075155009826\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 19 ---------\n",
      "train_loss: 0.028727359448870022\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 20 ---------\n",
      "train_loss: 0.027143389607469242\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 21 ---------\n",
      "train_loss: 0.025585550193985302\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 22 ---------\n",
      "train_loss: 0.02507374094178279\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 23 ---------\n",
      "train_loss: 0.025137557027240593\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 24 ---------\n",
      "train_loss: 0.02497273962944746\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 25 ---------\n",
      "train_loss: 0.02469439369936784\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 26 ---------\n",
      "train_loss: 0.024764885815481346\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 27 ---------\n",
      "train_loss: 0.024599548739691574\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 28 ---------\n",
      "train_loss: 0.024407124767700832\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 29 ---------\n",
      "train_loss: 0.024209719461699326\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 30 ---------\n",
      "train_loss: 0.024076057597994804\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 31 ---------\n",
      "train_loss: 0.02401783938209216\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 32 ---------\n",
      "train_loss: 0.02412033701936404\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 33 ---------\n",
      "train_loss: 0.024055547391374905\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 34 ---------\n",
      "train_loss: 0.02458073788632949\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 35 ---------\n",
      "train_loss: 0.02526910634090503\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 36 ---------\n",
      "train_loss: 0.025859168109794457\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 37 ---------\n",
      "train_loss: 0.025207711073259514\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 38 ---------\n",
      "train_loss: 0.024236951644221943\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 39 ---------\n",
      "train_loss: 0.02366242992381255\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 40 ---------\n",
      "train_loss: 0.02358063900222381\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 41 ---------\n",
      "train_loss: 0.023672703343133133\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 42 ---------\n",
      "train_loss: 0.023567961528897285\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 43 ---------\n",
      "train_loss: 0.02349662035703659\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 44 ---------\n",
      "train_loss: 0.023540714134772617\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 45 ---------\n",
      "train_loss: 0.02348920051008463\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 46 ---------\n",
      "train_loss: 0.023435492378969986\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 47 ---------\n",
      "train_loss: 0.023466586756209534\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 48 ---------\n",
      "train_loss: 0.023418564349412918\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 49 ---------\n",
      "train_loss: 0.02339310012757778\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 50 ---------\n",
      "train_loss: 0.023427497905989487\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 51 ---------\n",
      "train_loss: 0.023369547600547474\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 52 ---------\n",
      "train_loss: 0.02336830614755551\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 53 ---------\n",
      "train_loss: 0.023405331186950207\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 54 ---------\n",
      "train_loss: 0.023321771683792274\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 55 ---------\n",
      "train_loss: 0.023355507602294285\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 56 ---------\n",
      "train_loss: 0.023277281162639458\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 57 ---------\n",
      "train_loss: 0.023330426464478176\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 58 ---------\n",
      "train_loss: 0.02325835047910611\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 59 ---------\n",
      "train_loss: 0.023329622733096283\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 60 ---------\n",
      "train_loss: 0.023281073508163292\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 61 ---------\n",
      "train_loss: 0.02325942584623893\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 62 ---------\n",
      "train_loss: 0.023170145228505135\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 63 ---------\n",
      "train_loss: 0.02312652642528216\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 64 ---------\n",
      "train_loss: 0.02317329092572133\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 65 ---------\n",
      "train_loss: 0.02311827490727107\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 66 ---------\n",
      "train_loss: 0.02312197567274173\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 67 ---------\n",
      "train_loss: 0.023076632991433144\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 68 ---------\n",
      "train_loss: 0.02310455000648896\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 69 ---------\n",
      "train_loss: 0.02305572759360075\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 70 ---------\n",
      "train_loss: 0.023076101206243038\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 71 ---------\n",
      "train_loss: 0.02303101805349191\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 72 ---------\n",
      "train_loss: 0.023055981223781902\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 73 ---------\n",
      "train_loss: 0.023009334069987137\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 74 ---------\n",
      "train_loss: 0.023029298211137455\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 75 ---------\n",
      "train_loss: 0.0229826287056009\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 76 ---------\n",
      "train_loss: 0.02300448529422283\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 77 ---------\n",
      "train_loss: 0.02296046695361535\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 78 ---------\n",
      "train_loss: 0.0229787677526474\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 79 ---------\n",
      "train_loss: 0.02293476276099682\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 80 ---------\n",
      "train_loss: 0.022957467163602512\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 81 ---------\n",
      "train_loss: 0.02291716480006774\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 82 ---------\n",
      "train_loss: 0.02293462244172891\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 83 ---------\n",
      "train_loss: 0.022894081349174183\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 84 ---------\n",
      "train_loss: 0.022916666542490322\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 85 ---------\n",
      "train_loss: 0.022879708558321\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 86 ---------\n",
      "train_loss: 0.022895713957647484\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 87 ---------\n",
      "train_loss: 0.02285859143982331\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 88 ---------\n",
      "train_loss: 0.022880636155605316\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 89 ---------\n",
      "train_loss: 0.0228465823456645\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 90 ---------\n",
      "train_loss: 0.0228615819166104\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 91 ---------\n",
      "train_loss: 0.022827341221272945\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 92 ---------\n",
      "train_loss: 0.022848145415385563\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 93 ---------\n",
      "train_loss: 0.02281662356108427\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 94 ---------\n",
      "train_loss: 0.022830851065615814\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 95 ---------\n",
      "train_loss: 0.022799143257240456\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 96 ---------\n",
      "train_loss: 0.022818720278640587\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 97 ---------\n",
      "train_loss: 0.022789354746540386\n",
      "Save model and lr increase\n",
      "--------- module_EU_LG Epoch 98 ---------\n",
      "train_loss: 0.022802941190699737\n",
      "Restore model and lr decrease\n",
      "--------- module_EU_LG Epoch 99 ---------\n",
      "train_loss: 0.022773500221470993\n",
      "Save model and lr increase\n",
      "non acceptable module at max eps tensor([0.4038])\n",
      "////////// Start CRAM and REORG with unacceptable wt\n",
      "cramming sample 44th |0.00% total of 1\n",
      "tensor(0.0098)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(-0.0264)\n",
      "weird cram\n",
      "tensor([[33.9381],\n",
      "        [33.9418],\n",
      "        [33.9323],\n",
      "        [33.9448],\n",
      "        [33.9130],\n",
      "        [33.8974],\n",
      "        [33.8947],\n",
      "        [33.8925],\n",
      "        [33.8807],\n",
      "        [33.8806],\n",
      "        [34.0002],\n",
      "        [33.8721],\n",
      "        [34.0105],\n",
      "        [34.0150],\n",
      "        [34.0155],\n",
      "        [34.0165],\n",
      "        [34.0210],\n",
      "        [34.0317],\n",
      "        [33.8417],\n",
      "        [33.8375],\n",
      "        [34.0446],\n",
      "        [34.0453],\n",
      "        [33.8274],\n",
      "        [34.0480],\n",
      "        [34.0493],\n",
      "        [33.8208],\n",
      "        [34.0578],\n",
      "        [34.0599],\n",
      "        [34.0644],\n",
      "        [33.7930],\n",
      "        [34.0839],\n",
      "        [34.0859],\n",
      "        [34.0873],\n",
      "        [33.7844],\n",
      "        [34.0917],\n",
      "        [34.1023],\n",
      "        [34.1056],\n",
      "        [33.7676],\n",
      "        [33.7570],\n",
      "        [33.7564],\n",
      "        [34.1233],\n",
      "        [33.7463],\n",
      "        [33.7460],\n",
      "        [33.7391],\n",
      "        [33.7387],\n",
      "        [33.7348],\n",
      "        [33.7287],\n",
      "        [34.1506],\n",
      "        [34.1580],\n",
      "        [34.1795],\n",
      "        [33.6933],\n",
      "        [34.1845],\n",
      "        [34.1856],\n",
      "        [34.1864],\n",
      "        [34.1899],\n",
      "        [33.6828],\n",
      "        [33.6801],\n",
      "        [33.6728],\n",
      "        [33.6636],\n",
      "        [34.2127],\n",
      "        [33.6600],\n",
      "        [34.2193],\n",
      "        [33.6539],\n",
      "        [33.6415],\n",
      "        [33.9373]])\n"
     ]
    }
   ],
   "source": [
    "model = init_model(X_train, y_train)\n",
    "n = 0\n",
    "while n < 2000:\n",
    "    train_loader, indices_no_fit_well, n = lts(model, X_train, y_train, learning_goal)\n",
    "    acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "    if acceptable:\n",
    "        pass        \n",
    "    else:\n",
    "        torch.save(model, \"unacceptable/selecting_wt.pth\")\n",
    "        # store model in acceptable/wt.pth if acceptable\n",
    "        acceptable, model, train_loss_list, test_loss_list = \\\n",
    "            module_weight_EU_LG_UA(model, train_loader, test_loader, **config_wt)\n",
    "        if acceptable:\n",
    "            print(\"////////// Start REORG with accpetable wt //////////\")\n",
    "            # load model in acceptable/wt.pth if wt acceptable\n",
    "            pre_module = \"wt\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "        else:\n",
    "            print(\"////////// Start CRAM and REORG with unacceptable wt\")\n",
    "            # load model in unacceptable/selecting_wt.pth if wt not acceptable\n",
    "            model = torch.load(\"unacceptable/selecting_wt.pth\")            \n",
    "            cram = cramming(model, train_loader, X_train, y_train, indices_no_fit_well, **config_cram)\n",
    "            cram.cram()\n",
    "            model = cram.model\n",
    "            acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            if acceptable:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"weird cram\")\n",
    "                print(eps)\n",
    "                break\n",
    "\n",
    "            pre_module = \"cram\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(X_train, y_train)\n",
    "train_loader, indices_no_fit_well, n = lts(model, X_train, y_train, learning_goal)\n",
    "acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "indices_no_fit_well, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable, model, train_loss_list, test_loss_list = \\\n",
    "            module_weight_EU_LG_UA(model, train_loader, test_loader, **config_wt)\n",
    "acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3093])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"unacceptable/selecting_wt.pth\")  \n",
    "acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "eps[eps>learning_goal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3071]) tensor([0.3093])\n",
      "cramming sample 44th |0.00% total of 1\n",
      "tensor(0.0051)- Xk) != 0 and (s - r*(Xc - Xk))*(s + r*(Xc - Xk)) < 0\n",
      "tensor(0.0076)\n",
      "tensor(-0.0203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5777])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from module.Cram import *\n",
    "config_cram[\"s\"] = .1\n",
    "model = torch.load(\"unacceptable/selecting_wt.pth\")          \n",
    "acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)  \n",
    "print(max(eps - eps.min()), max(eps))\n",
    "cram = cramming(model, train_loader, X_train, y_train, indices_no_fit_well, **config_cram)\n",
    "cram.cram()\n",
    "model = cram.model\n",
    "acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)  \n",
    "max(eps - eps.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
