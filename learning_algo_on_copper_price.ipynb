{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from module.model import TwoLayerNet\n",
    "from module.Weight_tune import *\n",
    "from module.Reorg import *\n",
    "from module.Cram import *\n",
    "from module.init import *\n",
    "from module.lts import *\n",
    "from module.utils import *\n",
    "from module.data import *\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64\n",
    "\n",
    "data = pd.read_csv('Copper_forecasting_data.csv')\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = data.drop([\"y\"], axis = 1)\n",
    "X = sc.fit_transform(X)\n",
    "y = data[\"y\"] / 1000\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "batch_size = 30\n",
    "\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:] \n",
    "\n",
    "X_train = torch.tensor(np.array(X_train), dtype=dtype)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=dtype)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=dtype)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=dtype)\n",
    "input_dim = X_train.shape[1]\n",
    "    \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_test.to(device), y_test.to(device)), \n",
    "    batch_size = batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning goal: max eps < learning goal\n",
    "learning_goal = torch.exp(torch.tensor(1)).to(dtype = dtype)\n",
    "lr_rate = .01\n",
    "lr_bound = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "\"\"\"\n",
    "# Note\n",
    "1. hidden dim should check the previous model. dynamically change\n",
    "2. For lr_rate, lr bound, lr goal (eps bound) are all the same fro eahc module\n",
    "\"\"\"\n",
    "config_wt = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "}\n",
    "\n",
    "config_cram = {\n",
    "    \"lr_goal\": learning_goal, \n",
    "    \"s\": 0.001,                     # a small num in cram\n",
    "}\n",
    "\n",
    "config_reorg  = {\n",
    "    \"epochs\": epochs,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}\n",
    "# NOTE \n",
    "# 1. for leaning goals, if first using weightune and no LTS or otherthings, \n",
    "# 13 for getting acceptable wt | 10 not acceptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L11 p9, third learning mechanism\\\n",
    "`Check: should y add .reshape(-1, 1)? since some warning shows when testing the final model`\\\n",
    "`Check validate section`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the random of init. Should not be random in init.\n",
    "k = 0\n",
    "loss = []\n",
    "while k<2:\n",
    "    model = init_model(X_train, y_train)\n",
    "    # 2. obtaining_LTS / selecting_LTS\n",
    "    train_loader, indices, n = lts(model, X_train, y_train, learning_goal)\n",
    "\n",
    "    # 3. check learning goal\n",
    "    acceptable, model, train_loss_list, test_loss_list = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, **config_wt)\n",
    "    acceptable, eps, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "    loss.append(train_loss_list)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result log file\n",
    "file_create_time = str(datetime.datetime.now().date())\n",
    "out_file = open(f\"result_log/{file_create_time}\" + '.txt', 'a')\n",
    "out_file.write(f\"################################## New lts #######################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Full step for the learning algorithm mechanism\n",
    "# NOTE\n",
    "# 1. model right before reorg always need to be acceptable model\n",
    "# 2. model after cram and reorg always need to be acceptable \n",
    "# 3. check for the above every time after cram and reorg\n",
    "# 4. the randomness: in cram find r\n",
    "#################################################################\n",
    "\n",
    "# 1. initializing_1_ReLU_LR | L11 p2\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(str(datetime.datetime.now()))\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "out_file.write(\"##############################################################################\")\n",
    "model = init_model(X_train, y_train)\n",
    "n = 0\n",
    "n_not_fit = 5\n",
    "model_history = {}\n",
    "\n",
    "\n",
    "while n < len(X_train):\n",
    "    out_file.write(f\"### New lts ###\\n\")\n",
    "    models_within_iter = {}\n",
    "    # 2. obtaining_LTS / selecting_LTS\n",
    "    train_loader, indices, X_train_lts, y_train_lts, n = \\\n",
    "        lts(model, X_train, y_train, learning_goal, n_not_fit, out_file)\n",
    "\n",
    "    # 3. check learning goal\n",
    "    acceptable, eps_sqaure, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "    models_within_iter[\"begin\"] = model\n",
    "    if acceptable:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        torch.save(model, \"unacceptable/selecting.pth\")\n",
    "\n",
    "        # store model in acceptable/wt.pth if acceptable\n",
    "        # store model in unacceptable/wt.pth if not acceptable\n",
    "        out_file.write(f\"//////////// Start module_EU_LG_UA Epoch ///////////\\n\")\n",
    "        print(f\"//////////// Start module_EU_LG_UA Epoch ///////////\")\n",
    "        acceptable, model, train_loss_list, test_loss_list = \\\n",
    "            module_weight_EU_LG_UA(model, train_loader, test_loader, out_file, **config_wt)\n",
    "        models_within_iter[\"wt\"] = model\n",
    "        \n",
    "        if acceptable:\n",
    "            out_file.write(\"////////// Start REORG with accpetable wt //////////\\n\")   \n",
    "            out_file.write(f\"model after wt: {model}\\n\")\n",
    "            print(\"////////// Start REORG with accpetable wt //////////\")   \n",
    "            print(f\"model after wt: {model}\")\n",
    "            \n",
    "            # load model in acceptable/wt.pth if wt acceptable\n",
    "            pre_module = \"wt\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "            models_within_iter[\"reorg\"] = model\n",
    "            out_file.write(f\"model after reorg: {model}\\n\")\n",
    "            print(f\"model after reorg: {model}\")\n",
    "\n",
    "        else:\n",
    "            out_file.write(\"////////// Start CRAM and REORG with unacceptable wt //////////\\n\")\n",
    "            print(\"////////// Start CRAM and REORG with unacceptable wt //////////\")\n",
    "\n",
    "            # load model before wt: unacceptable/selecting.pth if unacceptable after wt\n",
    "            model = torch.load(\"unacceptable/selecting.pth\")    \n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            out_file.write(f\"model after wt: {model}\\n\")\n",
    "            out_file.write(f\"eps_square (last 10) before cram: {eps_square[-10:]}\\n\")\n",
    "            print(f\"model after wt: {model}\")\n",
    "            print(f\"eps_square (last 10) before cram: {eps_square[-10:]}\")\n",
    "\n",
    "            # load model in unacceptable/wt.pth if wt not acceptable       \n",
    "            # store acceptable cram in acceptable/cram.pth \n",
    "            cram = cramming(model, X_train[indices], y_train[indices], out_file, **config_cram)            \n",
    "            cram.cram() \n",
    "            model = cram.model\n",
    "            models_within_iter[\"cram\"] = model\n",
    "            out_file.write(f\"model after cram: {model}\\n\")  \n",
    "            print(f\"model after cram: {model}\")      \n",
    "            # check accetable\n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            out_file.write(f\"eps_sqaure (last 10) after cram: {eps_square[-10:]}\\n\")\n",
    "            print(f\"eps_sqaure (last 10) after cram: {eps_square[-10:]}\")\n",
    "            if not acceptable:\n",
    "                out_file.write(\"weird cram\\n\")\n",
    "                print(\"weird cram\")\n",
    "                break\n",
    "\n",
    "            # load model in acceptable/cram.pth if cram acceptable\n",
    "            # store acceptable cram in acceptable/cram.pth\n",
    "            pre_module = \"cram\"\n",
    "            reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "            reorg.reorganising()\n",
    "            model = reorg.model\n",
    "            models_within_iter[\"reorg\"] = model\n",
    "            out_file.write(f\"model after reorg: {model}\\n\")\n",
    "            print(f\"model after reorg: {model}\")\n",
    "\n",
    "            # check \n",
    "            acceptable, eps_square, y_pred = check_acceptable(train_loader, model, learning_goal)\n",
    "            if acceptable:\n",
    "                pass\n",
    "            else:\n",
    "                out_file.write(\"weird reorg\\n\")\n",
    "                print(\"weird reorg\")\n",
    "                break\n",
    "            \n",
    "    model_history[n] = models_within_iter\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model file\n",
    "import pickle\n",
    "with open(\"result/updated_models.pk\", \"wb\") as f:\n",
    "    pickle.dump(model_history, f)\n",
    "out_file.close()\n",
    "\n",
    "with open(\"result/updated_models.pk\", \"rb\") as f:\n",
    "    model_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all train loader\n",
    "triain_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(X_train.to(device), y_train.to(device)), \n",
    "    batch_size = batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5053799064934228, 19.585845570990124)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "train_loss = 0\n",
    "for x, y in train_loader:\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y.reshape(-1, 1))\n",
    "    train_loss += loss.item()\n",
    "train_loss/=len(train_loader)\n",
    "\n",
    "test_loss = 0\n",
    "for x, y in test_loader:\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y.reshape(-1, 1))\n",
    "    test_loss += loss.item()\n",
    "test_loss/=len(test_loader)\n",
    "\n",
    "train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.835551759396442, 887.2577571347302)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two layers\n",
    "bench_model = TwoLayerNet(18, 50, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(bench_model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(50):\n",
    "    for x, y in train_loader:\n",
    "        preds = bench_model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "train_loss = 0\n",
    "for x, y in train_loader:\n",
    "    output = bench_model(x).reshape(-1, 1)\n",
    "    loss = criterion(output, y.reshape(-1, 1))\n",
    "    train_loss += loss.item()\n",
    "train_loss/=len(train_loader)\n",
    "\n",
    "test_loss = 0\n",
    "for x, y in test_loader:\n",
    "    output = bench_model(x).reshape(-1, 1)\n",
    "    loss = criterion(output, y.reshape(-1, 1))\n",
    "    test_loss += loss.item()\n",
    "test_loss/=len(test_loader)\n",
    "\n",
    "train_loss, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
